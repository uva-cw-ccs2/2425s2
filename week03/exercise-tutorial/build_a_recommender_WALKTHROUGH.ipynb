{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96de7b20",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸŽ¬ Make your own recommender system!\n",
    "\n",
    "In this notebook, we'll build a simple but powerful **movie recommender** using different text similarity techniques. In this notebook, we will put all the pieces from the last three weeks together! More specifically, you will use your knowledge of preprocessing (week 1 and week 2), vectorizers (week 2), embeddings (week 2) and (soft) cosine similiarity (week 3) to build your own recommender system. \n",
    "\n",
    "\n",
    "We'll cover:\n",
    "\n",
    "-  A knowledge-based recommender using genres\n",
    "-  Content-based recommender systems using:\n",
    "   -  regular cosine similarity based on Count and TF-IDF vectorizers \n",
    "   - soft cosine similiarity based on spaCy embeddings for smart comparison\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c174d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc31cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04090771-7e63-4458-a91d-f5f931145ed2",
   "metadata": {},
   "source": [
    "### Explore and Preprocess the data\n",
    "\n",
    "Today, we will start working on building our own recommender system. For this assignment, we will work with movie data.\n",
    "Download the following datasets [here](https://www.kaggle.com/tmdb/tmdb-movie-metadata):\n",
    "- `tmdb_5000_credits.csv`\n",
    "- `tmdb_5000_movies.csv`\n",
    "\n",
    "Place the files a folder in the current working directory, which you can call `data/`.\n",
    "\n",
    "\n",
    "Let's explore the two datasets and identify what information is available and which columns may be useful for building a **knowledge-based** and **content-based recommender system**.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸŽ¬ `tmdb_5000_movies.csv`\n",
    "\n",
    "This dataset contains metadata for each movie, including:\n",
    "- `title`: Movie title\n",
    "- `genres`: JSON-formatted list of genres\n",
    "- `keywords`: Tags or themes for the movie\n",
    "- `overview`: A short description of the movie plot\n",
    "- `vote_average`, `vote_count`: Useful for understanding popularity\n",
    "- `runtime`, `release_date`, `popularity`\n",
    "- `production_companies`, `production_countries`\n",
    "- `original_language`\n",
    "- `id`: The unique movie ID (important for merging)\n",
    "\n",
    "Weâ€™ll likely focus on:\n",
    "- `title`, `overview`, `genres`, `keywords`, `vote_average`, `popularity`\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ‘¥ `tmdb_5000_credits.csv`\n",
    "\n",
    "This contains:\n",
    "- `movie_id`: Unique ID (can be matched with `id` in movies dataset)\n",
    "- `title`: Redundant but helpful for validation\n",
    "- `cast`: JSON-formatted list of cast members\n",
    "- `crew`: JSON-formatted list of crew members (can extract directors, writers, etc.)\n",
    "\n",
    "Weâ€™ll likely use:\n",
    "- `movie_id`, `cast`, `crew`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9fb716",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('data/tmdb_5000_movies.csv')\n",
    "credits = pd.read_csv('data/tmdb_5000_credits.csv')\n",
    "\n",
    "PATH = 'data/'\n",
    "\n",
    "VOTE_COUNT = 2000 #If you want to work with a larger dataset, decrease this value.\n",
    "\n",
    "def get_data(path_to_data):\n",
    "\n",
    "    data1 = pd.read_csv(f'{path_to_data}tmdb_5000_credits.csv')\n",
    "    data2 = pd.read_csv(f'{path_to_data}tmdb_5000_movies.csv')\n",
    "    data2.rename(columns={'id': 'movie_id'}, inplace=True)\n",
    "\n",
    "    data = pd.merge(data1,data2,  on=['movie_id', 'title'])\n",
    "    data[\"original_title\"] = data[\"original_title\"].str.lower()\n",
    "\n",
    "    data = data[data['vote_count'] > VOTE_COUNT] # for now, only keep movies with frequent votes (this will keep the dataset rather small and therefore computation is faster)\n",
    "    data.index = [i for i in range(0,len(data))]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044fff99-37d6-45cd-88e4-938669f7a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(PATH)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e483f2",
   "metadata": {},
   "source": [
    "# 1. Knowledge-based recommender system\n",
    "\n",
    "## Text Preprocessing\n",
    "\n",
    "As a first step, some data wrangling techniques are needed to get the data into the right shape.\n",
    "- Can you convert `release_year` to a yearly-level variable?\n",
    "- Can you clean up the `genres` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2275af8-0ade-44ed-a5dc-5a73d914fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract the 'release_year' from 'release_date'\n",
    "data['release_year'] = pd.to_datetime(data['release_date'], errors='coerce').dt.year\n",
    "\n",
    "# Step 2: Function to clean and combine genres into a simple text string\n",
    "def get_genres(x):\n",
    "    try:\n",
    "        # Convert the string of genres into a list, then join the names into one string (lowercase)\n",
    "        return \" \".join([genre['name'].lower() for genre in literal_eval(x)])\n",
    "    except (ValueError, SyntaxError):\n",
    "        # If there's an error in parsing, return an empty string\n",
    "        return ''\n",
    "\n",
    "# Step 3: Apply the function to the 'genres' column\n",
    "data['genres'] = data['genres'].apply(get_genres)\n",
    "\n",
    "# Step 4: Display the first few rows to check the result\n",
    "data.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72fa044-88b6-4042-86a4-6f44cd3beb64",
   "metadata": {},
   "source": [
    "\n",
    "-   In a knowledge-based recommender system, we leverage specific attributes of items (in this case, movies) to recommend similar items based on user preferences. One of the most useful attributes in movie recommendations is genre (e.g., Action, Comedy, Drama). In the next code, we will be 'exploding' the  genres column: The genres column contains a list of genres for each movie (e.g., `['Action', 'Adventure']`). To build a knowledge-based system, we need to break each list into individual genres so that each row in the data corresponds to a single genre for a movie. This allows us to make recommendations based on individual genres rather than a combination of genres.\n",
    "\n",
    "-   Creating a \"long\" format: The process of transforming data into this \"long\" format is called exploding. In the context of movie genres, each row will represent a movie and one of its genres. If a movie has multiple genres, there will be multiple rows for that movie (one for each genre)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005d8b4-6a6e-4162-9d77-ff9895904185",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = data.apply(lambda x: pd.Series(x['genres'].split()),axis=1).stack().reset_index(level=1, drop=True)\n",
    "s.name = 'single_genre'\n",
    "data = data.join(s)\n",
    "\n",
    "data[['single_genre', 'title', 'vote_average', 'vote_count', 'release_year']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44e2e80-1fcd-455e-b563-d147578f8f56",
   "metadata": {},
   "source": [
    "Now that the data looks in to be in a good shape, we can start with the actual recommendation system. We will use the following to get started with our knowledge-content-based recommendation system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb36366-85ca-40cf-971e-07d8351fbe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enter whatever:\")\n",
    "a_test = input()\n",
    "print(a_test) ## do you understand what happens here? Some input is taken from the user and stored in a_test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982b255",
   "metadata": {},
   "source": [
    "#### Example of a knowlege based recommender system\n",
    "\n",
    "Feel free to play around with this simple knowledge-based recommender system to see how it works and inspect the output! By experimenting with different genres, release years, and other preferences, you can explore how the system tailors movie recommendations based on your input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc614d-4046-4c89-beec-43b35c024054",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def knowledge_based_recommender(data):\n",
    "\n",
    "    data = data[data['single_genre'].notna()]\n",
    "    data['single_genre'] = data['single_genre'].str.lower()\n",
    "\n",
    "    print(f\"What type of genre do you like? \\n\\nYou can choose from the following:\\n\\n{set(data['single_genre'])}\")\n",
    "    genre = input().lower()\n",
    "\n",
    "    print(\"What is the minimum release year of movies you are interested in? (e.g., how 'old' may a movie be?)\" )\n",
    "    release_year = int(input())\n",
    "\n",
    "    movies = data[(data['single_genre'] == genre) &\n",
    "    (data['release_year'] >= release_year) ]\n",
    "\n",
    "    recommend_movies = movies.sort_values('vote_average', ascending=False)\n",
    "\n",
    "    return recommend_movies[['title', 'vote_average', 'genres']].head(5)\n",
    "\n",
    "\n",
    "knowledge_based_recommender(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b2ce67",
   "metadata": {},
   "source": [
    "# 2. Content-based recommender system\n",
    "\n",
    "## a. Content-based using Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdebdbf6",
   "metadata": {},
   "source": [
    "For this taks, we go back to the dataset in the original format (hence, before exploding the data to a long format).\n",
    "\n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **Create a Combined Feature Column**:\n",
    "   - We combine multiple text columns such as **overview**, **genres**, **tags**, etc., to create a new column called `combined_features`. This column will hold all the relevant textual data for each movie that will be used for similarity comparison.\n",
    "   - For example, combining the **overview** and **genres** might provide a richer context to better identify similar movies.\n",
    "\n",
    "2. **Preprocessing**: \n",
    "   - **Lowercase**: Convert the text to lowercase for uniformity.\n",
    "   - **Remove Punctuation**: Strip punctuation marks from the text.\n",
    "   - **Tokenization**: Split the text into individual words (tokens).\n",
    "   - **Stopwords Removal**: Remove common words like \"the\", \"is\", \"and\", etc., which do not contribute much to the meaning.\n",
    "   - **Stemming**: Reduce words to their root form (e.g., \"running\" â†’ \"run\").\n",
    "\n",
    "3. **User Input Preprocessing**:\n",
    "   - When a user enters a movie title, we preprocess the input text in the same way we processed the dataset, to ensure that the comparison is valid.\n",
    "\n",
    "4. **Vectorization**:\n",
    "   - Convert the text data in the `combined_features` column and user input into numerical vectors using **CountVectorizer**. This step transforms the text into a format that can be compared mathematically.\n",
    "\n",
    "5. **Cosine Similarity**:\n",
    "   - **Cosine similarity** is used to calculate how similar the user's input is to the movies in the dataset.\n",
    "   - The higher the cosine similarity, the more similar the movies are to the user's input.\n",
    "\n",
    "6. **Recommendation**:\n",
    "   - The system returns a list of the top 10 most similar movies based on the calculated cosine similarity.\n",
    "\n",
    "### Notes:\n",
    "- Creating the combined feature column allows the system to consider multiple aspects (overview, genres, etc.) when calculating similarity, which helps improve the quality of recommendations.\n",
    "- Preprocessing ensures that text variations (case, punctuation, etc.) do not affect the similarity calculation.\n",
    "- The **CountVectorizer** converts text into numerical form, which is required for comparing the user input with the movie dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ce5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(PATH)\n",
    "data['release_year'] = pd.DatetimeIndex(data['release_date']).year\n",
    "data['genres'] = data['genres'].apply(get_genres)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38540e93",
   "metadata": {},
   "source": [
    "### a. Create a combined feature column.\n",
    "\n",
    "\n",
    "The goal is to create a **combined feature column** by merging relevant columns in your dataset. This can improve your recommender system by providing a richer representation of each movie.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Combine columns like **`overview`**, and **`genres`**:\n",
    "\n",
    "```python\n",
    "data['combined'] = data[['genres', 'overview']].apply(lambda x: ','.join(x.dropna().astype(str)), axis=1)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9921a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(data): \n",
    "    data['combined_features'] = data[['original_title', 'genres', 'overview', 'tagline']].apply(lambda x: ','.join(x.dropna().astype(str)),axis=1)\n",
    "    return data\n",
    "\n",
    "data = combine_features(data)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17df61d",
   "metadata": {},
   "source": [
    "### b. Preprocess the data\n",
    "\n",
    "Before moving on to our vectorizers, we clean our created `combined_features` column. This is important because we want to make sure that our text data is in a format that can be easily processed by the vectorizers.\n",
    "\n",
    "Think about the following preprocessing steps from week 1 and week 2 of this course:\n",
    "\n",
    "- Lowercasing\n",
    "- Removing punctuation\n",
    "- Tokenizing\n",
    "- Removing stopwords (like \"the\", \"is\")\n",
    "- Stemming or Lemmatization (optional)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba6b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "# Apply preprocessing to the 'combined_feature' column\n",
    "data['processed_combined_features'] = data['combined_features'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6504b841",
   "metadata": {},
   "source": [
    "### c. Transform your data -- decide upon Count or Tfidf vectorizer. \n",
    "\n",
    "Think about a strategy for transforming your combined data column, as designed in the previous step. More specifically, `fit_transform` the combined data column using `tfidf` or `count` vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576251a-9041-4cb8-99f7-32b07071db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Vectorize the combined features\n",
    "vectorizer = CountVectorizer()  # or TfidfVectorizer() for TF-IDF\n",
    "vectors = vectorizer.fit_transform(data['processed_combined_features'])\n",
    "\n",
    "# Step 2: Compute cosine similarity between the movies\n",
    "cosine_sim = cosine_similarity(vectors)\n",
    "\n",
    "# Get the movie title from the user\n",
    "print(\"Welcome to the Movie Recommender!\")\n",
    "print(\"What movie do you like? Please enter the movie title:\")\n",
    "\n",
    "# Preprocess the user input\n",
    "user_input = input().strip().lower()\n",
    "user_input_processed = preprocess_text(user_input)  # Preprocess the input in the same way\n",
    "\n",
    "# Step 3: Vectorize the preprocessed user input\n",
    "user_input_vector = vectorizer.transform([user_input_processed])\n",
    "\n",
    "# Step 4: Compute cosine similarity between the user input and all movies\n",
    "cosine_sim_user = cosine_similarity(user_input_vector, vectors)\n",
    "\n",
    "# Step 5: Check if the movie exists in the dataset\n",
    "if user_input not in data['original_title'].str.lower().values:\n",
    "    print(\"Movie not in Database\")\n",
    "else:\n",
    "    # Find the index of the movie entered by the user\n",
    "    indices = pd.Series(data.index, index=data['original_title'].str.lower())\n",
    "    index = indices[user_input]\n",
    "\n",
    "    # Get similarity scores for the entered movie\n",
    "    sim_scores = list(enumerate(cosine_sim_user[0]))\n",
    "\n",
    "    # Sort the movies by similarity score\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top 10 most similar movies (excluding the movie itself)\n",
    "    sim_scores = sim_scores[1:11]  # Exclude the first movie, which is the same as the input movie\n",
    "\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Extract movie details for the recommendations\n",
    "    movie_id = data['movie_id'].iloc[movie_indices]\n",
    "    movie_title = data['original_title'].iloc[movie_indices]\n",
    "    movie_genres = data['genres'].iloc[movie_indices]\n",
    "\n",
    "    # Create a DataFrame for the recommendations\n",
    "    recommendation = pd.DataFrame(columns=['Id', 'title', 'genres'])\n",
    "    recommendation['Id'] = movie_id\n",
    "    recommendation['title'] = movie_title\n",
    "    recommendation['genres'] = movie_genres\n",
    "\n",
    "    # Display the recommendations\n",
    "    print(\"\\nHere are some movie recommendations based on your choice:\")\n",
    "    for index, row in recommendation.iterrows():\n",
    "        print(f\"Title: {row['title']}, Genres: {row['genres']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1afbb7a",
   "metadata": {},
   "source": [
    "\n",
    "## 2b. Content based using Soft Cosine -- spaCy Embeddings (No Preprocessing!)\n",
    "\n",
    "spaCy understands context â€” so we **don't need to preprocess**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464bac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Take user input for the movie description\n",
    "query_plot = input(\"What movie do you like? Please enter the movie description: \")\n",
    "\n",
    "# Process the user input (query) using spaCy\n",
    "query_doc = nlp(query_plot)\n",
    "\n",
    "# Process the 'combined_features' column for all movies in your dataset using spaCy\n",
    "doc_objects = [nlp(text) for text in data['combined_features']]\n",
    "\n",
    "# Calculate similarity scores between the user's query and each movie's combined features\n",
    "spacy_scores = [query_doc.similarity(doc) for doc in doc_objects]\n",
    "\n",
    "# Get the indices of the top 10 most similar movies\n",
    "sorted_indices = sorted(range(len(spacy_scores)), key=lambda i: spacy_scores[i], reverse=True)[:10]\n",
    "\n",
    "# Create a DataFrame to store the top 10 recommendations\n",
    "top_10_movies = pd.DataFrame({\n",
    "    'title': data['original_title'].iloc[sorted_indices],\n",
    "    'score': [spacy_scores[i] for i in sorted_indices]\n",
    "})\n",
    "\n",
    "# Display the top 10 most similar movies\n",
    "print(\"\\nTop 10 Most Similar Movies:\")\n",
    "for index, row in top_10_movies.iterrows():\n",
    "    print(f\"Title: {row['title']}, Similarity Score: {row['score']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
