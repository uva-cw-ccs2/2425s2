{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb0e531",
   "metadata": {},
   "source": [
    "# Getting some hands-on experience with supervised machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f47b20",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "In this tutorial we will work with supervised machine learning. We will classify tweets into four categories namely: `normal`, `abusive`, `hateful`, and `spam`.\n",
    "\n",
    "As you noted when reading the literature assigned for this and last week, there are a few steps that we need to take before we can use supervised machine learning. Namely:\n",
    "* Collect data (in CS often texts, e.g., tweets)\n",
    "* Develop a codebook and hand-code the data\n",
    "\n",
    "In this tutorial, we focus on the actual machine learning part of the process. Hence, we will use a dataset that already has been coded by humans. It contains tweets and each tweet has a label indicating to which of four categories it belongs, namely normal, abusive, hateful, or spam. Hence, we skip the first two steps of the process described above.\n",
    "\n",
    "Download the data for this exercise named \"hatespeech_text_label_vote_RESTRICTED_100K.csv\". These datafiles were retrieved from: https://www.dropbox.com/sh/4mapojr85a6sc76/AABYMkjLVG-HhueAgd0qM9kwa?dl=0\n",
    "\n",
    "Using the examples from past lectures, can you write a script that opens each file and:\n",
    "* Creates one list with the tweets\n",
    "* Creates one list with the labels of the tweets\n",
    "\n",
    "What could you do to check that this process went well? Can you explore the data a bit (i.e., by checking how often each label is present in the different datasets)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec79bdc8-018e-48f8-8af5-d127ab32aa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 Tweets:\n",
      " ['Beats by Dr. Dre urBeats Wired In-Ear Headphones - White https://t.co/9tREpqfyW4 https://t.co/FCaWyWRbpE', 'RT @Papapishu: Man it would fucking rule if we had a party that was against perpetual warfare.', 'It is time to draw close to Him &#128591;&#127995; Father, I draw near to you now and always ❤️ https://t.co/MVRBBX2aqJ']\n",
      "\n",
      "First 3 Labels:\n",
      " ['spam', 'abusive', 'normal']\n"
     ]
    }
   ],
   "source": [
    "### Model answer\n",
    "\n",
    "import csv\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "file = \"hatespeech_text_label_vote_RESTRICTED_100K.csv\"\n",
    "tweets = []\n",
    "labels = []\n",
    "\n",
    "with open(file) as fi:\n",
    "    data = csv.reader(fi, delimiter='\\t')\n",
    "    for row in data:\n",
    "        tweets.append(row[0])\n",
    "        labels.append(row[1])\n",
    "\n",
    "print(\"First 3 Tweets:\\n\", tweets[:3])\n",
    "print(\"\\nFirst 3 Labels:\\n\", labels[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcf390dd-bc57-44db-998e-9dec18753092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(tweets) == len(labels)) # there should be just as many tweets as there are labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74268e0c-4886-42a0-b0b5-4006a46b18b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Counter(labels)\n",
    "plt.bar(Counter(labels).keys(), Counter(labels).values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2112676",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Now that we have read in the data, we will proceed to the next step: Splitting our data into a training set and a test set. Luckily, scikit-learn has a function that can do so for us! Run the code presented in the next block to split up the dataset.\n",
    "        \n",
    "* What do these lines of code do?\n",
    "* Do you know what the random_state part refers to? Why is this useful?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b45e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tweets_train, tweets_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344af64f-c7f6-4418-9086-283d7782402c",
   "metadata": {},
   "source": [
    "**Model answer**\n",
    "\n",
    "This function splits the dataset into a train set (80% of the data) and a test set (20% of the data). The `random_state` argument can be used to keep the same results the next time you run this same function. This can be useful if you want to reproduce your results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff085196",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "On to the next step: Transforming the text into numbers, or setting up a vectorizer. Can you create some code that uses a count vectorizer on the texts that you read in? Hint: check out the example provided in the slides of previous weeks! Doing so, you will see that the stopwords are defined (as a built-in stop word list). Why is that done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeaec147-2f15-4505-bfe3-29a946b5578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model answer\n",
    "\n",
    "from sklearn.feature_extraction.text import (CountVectorizer)\n",
    "\n",
    "countvectorizer = CountVectorizer(stop_words=\"english\")\n",
    "X_train = countvectorizer.fit_transform(tweets_train)\n",
    "X_test = countvectorizer.transform(tweets_test)\n",
    "\n",
    "# Stopwords are defined so that stopwords can be identified and excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e30d61e",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Now, let’s train a classifier and run it on the test data! Can you use the example from this week's lecture to train a Naïve Bayes classifer with our count vectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "145ebc6b-810d-4805-9fd0-1f8dea723c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model answer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177430c6",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "When you run the code you created for the previous question, you will see that it prints no output. How do you know if your code worked? Run the code presented in the next block (depending on how you named your labels, you may need to adjust the arguments).\n",
    "\n",
    "Check out the documentation of the scikit learn package: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "\n",
    "What do the numbers in the output mean? What can you do with it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86eb3389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['normal' 'normal' 'normal' 'normal' 'spam' 'normal' 'normal' 'normal'\n",
      " 'abusive' 'normal']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.81      0.88      0.85      5369\n",
      "     hateful       0.83      0.05      0.10       966\n",
      "      normal       0.78      0.93      0.85     10848\n",
      "        spam       0.67      0.30      0.41      2817\n",
      "\n",
      "    accuracy                           0.78     20000\n",
      "   macro avg       0.77      0.54      0.55     20000\n",
      "weighted avg       0.78      0.78      0.75     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(y_pred[:10])\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d97751",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Our last classifier was based on a count vectorizer using Naïve Bayes. Can you now train another classifier based on `Logistic Regression` and a `tf-idf vectorizer`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06651f97-ccb0-4baf-9e55-8cdd045a6b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.87      0.90      0.88      5369\n",
      "     hateful       0.66      0.25      0.37       966\n",
      "      normal       0.82      0.89      0.85     10848\n",
      "        spam       0.57      0.48      0.52      2817\n",
      "\n",
      "    accuracy                           0.80     20000\n",
      "   macro avg       0.73      0.63      0.66     20000\n",
      "weighted avg       0.79      0.80      0.79     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Model answer\n",
    "\n",
    "from sklearn.feature_extraction.text import (TfidfVectorizer)\n",
    "from sklearn.linear_model import (LogisticRegression)\n",
    "\n",
    "\n",
    "Tfidfvectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X_train = Tfidfvectorizer.fit_transform(tweets_train)\n",
    "X_test = Tfidfvectorizer.transform(tweets_test)\n",
    "\n",
    "logres = LogisticRegression(max_iter=1000)\n",
    "logres.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logres.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09fe6a2",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "As you saw in the article by Meppelink et al. (2021), we can try different combinations of these models (Naïve Bayes and Logistic Regression) and vectorizers (count and tf-idf). If you want to use Naïve Bayes and Logistic Regression as the models for a classifier, and a count vectorizer and a tf-idf vectorizer, how many classifiers could you then train? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54cce13-2183-479e-87e5-221546535e0d",
   "metadata": {},
   "source": [
    "**Model Answer**: You could then train 2 (count vectorzier vs. tf-idf vectorizer) * 2 (Naïve Bayes vs. Logistic Regression) = 4 classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e29260",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "If we want to train multiple different classifiers, we could simply copy-paste the code used in the previous questions and adjust it for each of the classifiers. However, a cleaner approach is to write a function in which we define the specifics of each classifier. The code below does that.\n",
    "\n",
    "In this code, we create a loop that trains each classifier by calling the function that is built in the first part of the code. \n",
    "\n",
    "Run the code below and compare it to the code that you wrote to train one classifier: Do you understand what is happening there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a09348f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB-count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.80      0.88      0.84      5369\n",
      "     hateful       0.41      0.28      0.33       966\n",
      "      normal       0.85      0.79      0.82     10848\n",
      "        spam       0.53      0.63      0.57      2817\n",
      "\n",
      "    accuracy                           0.77     20000\n",
      "   macro avg       0.65      0.64      0.64     20000\n",
      "weighted avg       0.77      0.77      0.77     20000\n",
      "\n",
      "NB-TfIdf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.81      0.81      0.81      5369\n",
      "     hateful       0.87      0.05      0.09       966\n",
      "      normal       0.76      0.92      0.83     10848\n",
      "        spam       0.65      0.32      0.43      2817\n",
      "\n",
      "    accuracy                           0.77     20000\n",
      "   macro avg       0.77      0.53      0.54     20000\n",
      "weighted avg       0.76      0.77      0.73     20000\n",
      "\n",
      "LR-Count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.87      0.91      0.89      5369\n",
      "     hateful       0.63      0.28      0.38       966\n",
      "      normal       0.81      0.91      0.86     10848\n",
      "        spam       0.58      0.39      0.47      2817\n",
      "\n",
      "    accuracy                           0.80     20000\n",
      "   macro avg       0.72      0.62      0.65     20000\n",
      "weighted avg       0.79      0.80      0.79     20000\n",
      "\n",
      "LR-TfIdf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.88      0.89      0.89      5369\n",
      "     hateful       0.73      0.20      0.32       966\n",
      "      normal       0.80      0.94      0.86     10848\n",
      "        spam       0.66      0.35      0.46      2817\n",
      "\n",
      "    accuracy                           0.81     20000\n",
      "   macro avg       0.77      0.60      0.63     20000\n",
      "weighted avg       0.80      0.81      0.78     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "configs = [\n",
    "    (\"NB-count\", CountVectorizer(min_df=5, max_df=.5), MultinomialNB()),\n",
    "    (\"NB-TfIdf\", TfidfVectorizer(min_df=5, max_df=.5), MultinomialNB()),\n",
    "    (\"LR-Count\", CountVectorizer(min_df=5, max_df=.5), LogisticRegression(solver=\"liblinear\")),\n",
    "    (\"LR-TfIdf\", TfidfVectorizer(min_df=5, max_df=.5), LogisticRegression(solver=\"liblinear\"))\n",
    "]\n",
    "\n",
    "for name, vectorizer, classifier in configs:\n",
    "    print(name)\n",
    "    X_train = vectorizer.fit_transform(tweets_train)\n",
    "    X_test = vectorizer.transform(tweets_test)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86c8380",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n",
    "Check out the documentation of scikit learn (https://scikit-learn.org/stable/supervised_learning.html). Can you try to use other models and train a classifier with them? Can you merge this code into the code used in the previous question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2b08d0a-d297-4b0c-8a6c-c520011b8d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB-count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.80      0.88      0.84      5369\n",
      "     hateful       0.41      0.28      0.33       966\n",
      "      normal       0.85      0.79      0.82     10848\n",
      "        spam       0.53      0.63      0.57      2817\n",
      "\n",
      "    accuracy                           0.77     20000\n",
      "   macro avg       0.65      0.64      0.64     20000\n",
      "weighted avg       0.77      0.77      0.77     20000\n",
      "\n",
      "\n",
      "\n",
      "NB-TfIdf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.81      0.81      0.81      5369\n",
      "     hateful       0.87      0.05      0.09       966\n",
      "      normal       0.76      0.92      0.83     10848\n",
      "        spam       0.65      0.32      0.43      2817\n",
      "\n",
      "    accuracy                           0.77     20000\n",
      "   macro avg       0.77      0.53      0.54     20000\n",
      "weighted avg       0.76      0.77      0.73     20000\n",
      "\n",
      "\n",
      "\n",
      "LR-count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.87      0.91      0.89      5369\n",
      "     hateful       0.63      0.28      0.38       966\n",
      "      normal       0.81      0.91      0.86     10848\n",
      "        spam       0.58      0.39      0.47      2817\n",
      "\n",
      "    accuracy                           0.80     20000\n",
      "   macro avg       0.72      0.62      0.65     20000\n",
      "weighted avg       0.79      0.80      0.79     20000\n",
      "\n",
      "\n",
      "\n",
      "LR-TfIdf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.88      0.89      0.89      5369\n",
      "     hateful       0.73      0.20      0.32       966\n",
      "      normal       0.80      0.94      0.86     10848\n",
      "        spam       0.66      0.35      0.46      2817\n",
      "\n",
      "    accuracy                           0.81     20000\n",
      "   macro avg       0.77      0.60      0.63     20000\n",
      "weighted avg       0.80      0.81      0.78     20000\n",
      "\n",
      "\n",
      "\n",
      "KNN-count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.60      0.84      0.70      5369\n",
      "     hateful       0.40      0.14      0.21       966\n",
      "      normal       0.74      0.52      0.61     10848\n",
      "        spam       0.27      0.42      0.33      2817\n",
      "\n",
      "    accuracy                           0.57     20000\n",
      "   macro avg       0.50      0.48      0.46     20000\n",
      "weighted avg       0.62      0.57      0.58     20000\n",
      "\n",
      "\n",
      "\n",
      "KNN-tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.78      0.44      0.56      5369\n",
      "     hateful       0.40      0.13      0.19       966\n",
      "      normal       0.64      0.48      0.55     10848\n",
      "        spam       0.17      0.52      0.25      2817\n",
      "\n",
      "    accuracy                           0.46     20000\n",
      "   macro avg       0.50      0.39      0.39     20000\n",
      "weighted avg       0.60      0.46      0.49     20000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Model answer\n",
    "\n",
    "# Various answers are possible here, but let's try a model based on K nearest neighbours for example:\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "configs = [\n",
    "  (\"NB-count\",CountVectorizer(min_df=5,max_df=.5),\n",
    "   MultinomialNB()),\n",
    "  (\"NB-TfIdf\",TfidfVectorizer(min_df=5,max_df=.5),\n",
    "   MultinomialNB()),\n",
    "  (\"LR-count\",CountVectorizer(min_df=5,max_df=.5),\n",
    "   LogisticRegression(solver=\"liblinear\")),\n",
    "  (\"LR-TfIdf\",TfidfVectorizer(min_df=5,max_df=.5),\n",
    "   LogisticRegression(solver=\"liblinear\")),\n",
    "  (\"KNN-count\",CountVectorizer(min_df=5,max_df=.5),\n",
    "  KNeighborsClassifier(n_neighbors=3)),\n",
    "  (\"KNN-tfidf\",TfidfVectorizer(min_df=5,max_df=.5),\n",
    "  KNeighborsClassifier(n_neighbors=3))]\n",
    "\n",
    "\n",
    "for name, vectorizer, classifier in configs:\n",
    "    print(name)\n",
    "    X_train = vectorizer.fit_transform(tweets_train)\n",
    "    X_test = vectorizer.transform(tweets_test)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\") \n",
    "\n",
    "\n",
    "# Note that only the first part of the code needs to be adjusted!\n",
    "# Also note that running the KNN classifier takes a bit of time..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf209f7e",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "\n",
    "Based on the output that the classifier prints, what classifier performs the best? In your answer, consider:\n",
    "* What information you need to identify the best classifier\n",
    "* What metric you base your conclusion (i.e., precision, recall, accurcay, or F1-score) on and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df6d7b2-6db8-4c8c-a795-c3fcd79c4d24",
   "metadata": {},
   "source": [
    "**Model answer**\n",
    "\n",
    "Depending on the metric that you base your evaluation on, a different classifier could be considered best. The best metric in turn, differs depending on what a classifier is used for.\n",
    "\n",
    "In this case, no specific information about the usage of the classifier is provided. Therefore, the F1-score may be the best metric to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977fce73",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "\n",
    "Let's say that you base your evaluation on the F1-score of the classifier. You can choose between the macro average and the weighted average of the F1-score. Check out the scikit learn documentation (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html). What F1-value (macro average or weighted average) would you select?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5411e0c-4dd8-4689-a655-7a331c45ce13",
   "metadata": {},
   "source": [
    "**Model answer**\n",
    "\n",
    "From the documentation:\n",
    "\n",
    "'macro': Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "\n",
    "'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F1-score that is not between precision and recall. We saw earlier that there are relatively many normal tweets present in the dataset. To account for this, the weighted average may be a good choide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92830270",
   "metadata": {},
   "source": [
    "## Question 12\n",
    "\n",
    "When looking at the classification report, you will see another column indicating values for something labelled 'support'.\n",
    "Can you do some searching online and find out what 'support' is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464f8282-12d9-4db6-8db1-3a8a03de14c6",
   "metadata": {},
   "source": [
    "**Model answer**: Support refers to the number of true instances for each label ([reference](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe8fa3",
   "metadata": {},
   "source": [
    "## Question 13\n",
    "\n",
    "Two researchers want to use the classifier to distinguish between tweets that are spam or hateful and tweets that are not (either because they are normal or abusive). \n",
    "They are, however, not happy with the performance of the classifier when looking at the accuracy, precision, and recall for the spam category or the hateful category.\n",
    "One of the researchers suggests to first recode the labels, so that all tweets that were annotated as spam receive a label 'spam' or 'hateful' are grouped together and all other tweets are grouped together as well.\n",
    "\n",
    "What would the consequences be of doing so? What can you do to check your own answer? Try to recode the labels and see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885ed529-cdb9-4d07-96f4-f64ac9b8d65b",
   "metadata": {},
   "source": [
    "**Model answer**\n",
    "\n",
    "It could increase the performance of the machine because the groups are no made larger and more easy to dinstinguish... Or the groups are made too larger and therefore too vague, making it harder to distinguish between categories, decreasing the performance of the machine...\n",
    "\n",
    "You can recode the labels and train/validate the machines again to see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80781bc5",
   "metadata": {},
   "source": [
    "## Question 14\n",
    "\n",
    "For now, let's say that the classifier based on a count vectorizer and Logistic Regression is the one we prefer. We now want to use this model to predict the label for new data that we have not annotated (remember, this was the whole goal of SML)!\n",
    "\n",
    "To do this, let’s save our classifier and our vectorizer to a file. If we don’t do this, we would need to re-train our model every time we want to use it. This is not so convenient, for example, we would always need to have our training data at hand. The code below shows you how to make a vectorizer and train a classifier (a repetition of what we did before to show you the whole process) and finally store them into a file for future use.\n",
    "\n",
    "In the code, you will see that both the classifier and the vectorizer are stored into a file. Why do you need to store both (why not just store the classifier only)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "858d4a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This Tweet is very shitty nasty mean and hateful' is probably 'abusive'.\n",
      "'This is a very normal normal tweet.' is probably 'normal'.\n",
      "'2%^&GHJ &(&hrqjf3 click this link' is probably 'spam'.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Make a vectorizer and train a classifier\n",
    "vectorizer=CountVectorizer(min_df=5, max_df=.5)\n",
    "classifier=LogisticRegression(solver=\"liblinear\")\n",
    "X_train=vectorizer.fit_transform(tweets_train)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Save them to disk\n",
    "with open(\"myvectorizer.pkl\",mode=\"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "with open(\"myclassifier.pkl\",mode=\"wb\") as f:\n",
    "    joblib.dump(classifier, f)\n",
    "\n",
    "# Later on, re-load this classifier and apply:\n",
    "new_tweets = [\"This Tweet is very shitty nasty mean and hateful\", \n",
    "            \"This is a very normal normal tweet.\", \n",
    "            \"2%^&GHJ &(&hrqjf3 click this link\"]\n",
    "\n",
    "with open(\"myvectorizer.pkl\",mode=\"rb\") as f:\n",
    "    myvectorizer = pickle.load(f)\n",
    "with open(\"myclassifier.pkl\",mode=\"rb\") as f:\n",
    "    myclassifier = joblib.load(f)\n",
    "    \n",
    "new_features = myvectorizer.transform(new_tweets)\n",
    "pred = myclassifier.predict(new_features)\n",
    "\n",
    "for tweet, label in zip(new_tweets, pred):\n",
    "    print(f\"'{tweet}' is probably '{label}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e594404-4ac4-4bb4-abcd-e77e548a1b8b",
   "metadata": {},
   "source": [
    "The vectorizer needs to be stored as well. Remember what we talked about earlier: once a vectorizer is fit on certain data, fitting it again on new data changes the set up of the vectorizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c74415b-e596-4bae-b4db-aa3746356c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
