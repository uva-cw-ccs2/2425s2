{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e4aa57f",
   "metadata": {},
   "source": [
    "# Understanding vectorizers\n",
    "\n",
    "In the following code examples, we will experiment with vectorizers to understand a bit better how they work. Feel free to adjust the code, and try things out yourself.\n",
    "\n",
    "For now, we will practice with `sklearn`'s vectorizers. However, packages such as `gensim` offer their own built-in functionality to vectorize the data. \n",
    "You can also play around [here](https://apps-computational-teaching-jj92xohbgnwnhksxlclr8t.streamlit.app/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29eb18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02d45caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0372eb2",
   "metadata": {},
   "source": [
    "## Example 1: Inspect the output of a vectorizer in a dense format\n",
    "\n",
    "The following code cell will fit and transform three documents using a `Count`-based vectorizer. Next, the output is transformed to a *dense* matrix, and printed.\n",
    "\n",
    "### 1. Do you understand the output?\n",
    "\n",
    "**Answer:**  \n",
    "The output will be a matrix of numbers. Each row represents one document from the list `texts`, and each column corresponds to a unique word (after the vectorizer processes the documents). The numbers in the matrix represent how important each word is in the context of each document.\n",
    "\n",
    "- With **`CountVectorizer()`**, the numbers will represent the raw **frequency** of each word in the documents.\n",
    "- With **`TfidfVectorizer()`**, the numbers will represent the **TF-IDF score** (Term Frequency-Inverse Document Frequency). The TF-IDF score reflects how important a word is within a particular document, considering how often it appears and how unique it is across all documents. Words that appear frequently in one document but rarely across the others will have higher values.\n",
    "\n",
    "\n",
    "### 2. Is it smart to transform output to a dense format? What will happen if you work with millions of documents, rather than 3 short sentences?\n",
    "\n",
    "**Answer:**  \n",
    "While itâ€™s okay for small datasets (like our 3 short sentences), transforming the output to a dense matrix can be very memory-intensive for large datasets. A dense matrix stores every value, even if many of them are zero. For millions of documents, this approach could use a lot of memory and lead to performance issues.\n",
    "\n",
    "In practice, we often use sparse matrices for large datasets, which only store non-zero values, making them much more memory-efficient. Scikit-learn, by default, uses sparse matrices when working with `CountVectorizer()` and `TfidfVectorizer()`.\n",
    "\n",
    "\n",
    "### 3. What happens if you replace `CountVectorizer()` for `TfidfVectorizer()`?\n",
    "\n",
    "**Answer:**  \n",
    "Both `CountVectorizer()` and `TfidfVectorizer()` convert text into numeric vectors, but the main difference is in how they treat word importance:\n",
    "\n",
    "- `CountVectorizer()` simply counts how many times each word appears in each document.\n",
    "- `TfidfVectorizer()` adjusts the counts by taking into account both the frequency of the word in a document (**TF**) and how common the word is across all documents (**IDF**). Words that are common across all documents (like \"the\", \"and\", etc.) get a lower score, while words that are more unique to specific documents get higher scores.\n",
    "\n",
    "So, if you replace `CountVectorizer()` with `TfidfVectorizer()`, the numbers in the resulting matrix will reflect word importance, rather than just word frequency. This is useful when you want to emphasize the significance of unique terms in a dataset, which can be helpful for many machine learning tasks like classification or clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe0a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"hello students!\", \"how are you today?\", \"what?\", \"hello hello everybody\"]\n",
    "vect = CountVectorizer()\n",
    "#vect = TfidfVectorizer()# initialize the vectorizer\n",
    "X = vect.fit_transform(texts) #fit the vectorizer and transform the documents in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a8609a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   are  everybody  hello  how  students  today  what  you\n",
      "0    0          0      1    0         1      0     0    0\n",
      "1    1          0      0    1         0      1     0    1\n",
      "2    0          0      0    0         0      0     1    0\n",
      "3    0          1      2    0         0      0     0    0\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(X.toarray(), columns=vect.get_feature_names_out()).to_string())\n",
    "df = pd.DataFrame(X.toarray().transpose(), index = vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c0c0c5",
   "metadata": {},
   "source": [
    "## Example 2: Inspect the output of a vectorizer in a sparse format\n",
    "\n",
    "Internally, `sklearn` represents the data in a *sparse* format, as this is computationally more efficient, and less memory is required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea857d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"hello students!\", \"how are you today?\", \"what?\", \"hello hello everybody\"]\n",
    "count_vec = CountVectorizer() #initilize the vectorizer\n",
    "count_vec_fit = count_vec.fit_transform(texts) #fit the vectorizer and transform the documents in one go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03354120",
   "metadata": {},
   "source": [
    "1. Inspect the shape of transformed texts. We can see that we have a 4x8 sparse matrix, meaning that we have 4 \n",
    "rows (=documents) and 8 unique tokens (=words, numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe603bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cd6f01",
   "metadata": {},
   "source": [
    "2. Get the feature names. This will return the tokens that are in the vocabulary of the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a925f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['are', 'everybody', 'hello', 'how', 'students', 'today', 'what',\n",
       "       'you'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb2c6f3",
   "metadata": {},
   "source": [
    "3. Represent the token's mapping to its id values. The numbers do *not* represent the count of the words but the position of the words in the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0937869d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 2,\n",
       " 'students': 4,\n",
       " 'how': 3,\n",
       " 'are': 0,\n",
       " 'you': 7,\n",
       " 'today': 5,\n",
       " 'what': 6,\n",
       " 'everybody': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.vocabulary_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b768f3b",
   "metadata": {},
   "source": [
    "4. Get sparse representation on document level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62d39871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current document: hello students!\n",
      "  (0, 2)\t1\n",
      "  (0, 4)\t1\n",
      "\n",
      "Current document: how are you today?\n",
      "  (0, 3)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 5)\t1\n",
      "\n",
      "Current document: what?\n",
      "  (0, 6)\t1\n",
      "\n",
      "Current document: hello hello everybody\n",
      "  (0, 2)\t2\n",
      "  (0, 1)\t1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, document in zip(count_vec_fit, texts):\n",
    "    print(f\"Current document: {document}\")\n",
    "    print(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4a49e",
   "metadata": {},
   "source": [
    "## a. Do you understand the output printed above?\n",
    "\n",
    "**Answer:**  \n",
    "The output consists of the original document and its corresponding sparse vector (the transformed representation) printed for each document. The sparse vector format shows the index and frequency of the words in the document, represented in a compact form. Here's a sample output explanation based on the code snippet:\n",
    "\n",
    "For a list of documents like `[\"hello students!\", \"how are you today?\", \"what?\", \"hello hello everybody\"]`, the output would look something like this:\n",
    "\n",
    "- **Current document: `hello students!`**  \n",
    "  `(0, 2) 1 \n",
    "  (0, 4) 1`  \n",
    "  *(This is a sparse vector representation of the document. It means that in the vector, the word at index 2 and index 4 each appear once.)*\n",
    "\n",
    "- **Current document: `how are you today?`**  \n",
    "  `(0, 3) 1 (0, 0) 1 (0, 7) 1 (0, 5) 1`  \n",
    "  *(For this document, words at indices 0, 3, 5, and 7 appear once.)*\n",
    "\n",
    "- **Current document: `what?`**  \n",
    "  `(0, 6) 1`  \n",
    "  *(This document has one word, located at index 6.)*\n",
    "\n",
    "- **Current document: `hello hello everybody`**  \n",
    "  `(0, 2) 2 (0, 1) 1`  \n",
    "  *(In this document, the word at index 2 appears twice, while the word at index 1 appears once.)*\n",
    "\n",
    "Each vector is a sparse array representing the frequency of words (in this case, the raw counts) in the document. The format `(0, x) y` indicates that in the sparse matrix, the word at index `x` occurs `y` times in the document. This is a compact way of storing the data, as it only records the positions and frequencies of non-zero elements, which saves space for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## b. What happens if you change the `count` to a `tfidf` vectorizer?\n",
    "\n",
    "**Answer:**  \n",
    "If you replace the `CountVectorizer` with a `TfidfVectorizer`, the transformation of the documents changes as follows:\n",
    "\n",
    "- **CountVectorizer**: It counts the frequency of each word in the document. The output vector is a simple count of the words in the document.\n",
    "  \n",
    "  Example:  \n",
    "  Document: \"hello students\"  \n",
    "  Vector (CountVectorizer): [1, 1, 0, 0, 0, ...]  *(Count of each word in the document)*\n",
    "\n",
    "- **TfidfVectorizer**: It computes the Term Frequency-Inverse Document Frequency (TF-IDF) for each word. The output vector is adjusted to emphasize the importance of words that are more unique to a specific document (higher TF-IDF value). Words that occur frequently across all documents are given a lower score.\n",
    "\n",
    "  Example:  \n",
    "  Document: \"hello students\"  \n",
    "  Vector (TfidfVectorizer): [0.5, 0.5, 0.0, 0.0, 0.0, ...]  *(TF-IDF score for each word)*\n",
    "\n",
    "The difference:  \n",
    "- **CountVectorizer**: Simply counts the word occurrences.  \n",
    "- **TfidfVectorizer**: Adjusts the word counts by considering the frequency of the word across all documents, making it more sensitive to the importance of words in individual documents.\n",
    "\n",
    "So, when you replace the `CountVectorizer` with the `TfidfVectorizer`, the numbers in the printed vectors (such as `i`) will represent the TF-IDF scores instead of raw counts. The vectors for words that are more common across documents will have lower values, while unique words will have higher values.</span>\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- **CountVectorizer**: Output vector contains raw word counts.  \n",
    "- **TfidfVectorizer**: Output vector contains TF-IDF scores, which balance word frequency and document uniqueness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02771f7f",
   "metadata": {},
   "source": [
    "5. Get some final descriptives about the sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d845b132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-zero elements: 9\n",
      "Total number of elements: 32\n",
      "Sparsity: 0.6875\n"
     ]
    }
   ],
   "source": [
    "nonzero = df.astype(bool).sum(axis=0)\n",
    "print(\"Number of non-zero elements:\", nonzero.sum())\n",
    "print(\"Total number of elements:\", count_vec_fit.shape[0] * count_vec_fit.shape[1])\n",
    "\n",
    "# compute the sparsity of the matrix: w the proportion of zero elements in the matrix\n",
    "print(\"Sparsity:\", 1 - count_vec_fit.sum() / (count_vec_fit.shape[0] * count_vec_fit.shape[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
