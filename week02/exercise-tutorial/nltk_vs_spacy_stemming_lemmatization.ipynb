{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and lemmatization (using NLTK vs. spaCy)\n",
    "\n",
    "This notebook shows how different lemmatizers and stemmer algorithms work. It compares NLTK and spaCy methods for turning words into their base forms. You'll see how stemming chops words down quickly (but sometimes messily), while lemmatization uses dictionaries to get more accurate results. Run the code and see for yourself how each technique handles the same text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "# !pip install nltk spacy\n",
    "# !python -m nltk.downloader wordnet punkt\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = (\n",
    "    \"The striped bats were hanging on their feet for the best outcomes. \"\n",
    "    \"They had been running, thinking, and eating quickly. \"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Stemming using Porter & Snowball stemmers\n",
    "-   \n",
    "    *What it does*: Applies simple rule-based suffix stripping to reduce words to their base form.  \n",
    "    *Pros*: Fast and lightweight.  \n",
    "    *Cons*: May result in non-words or inconsistent roots (e.g., \"studies\" → \"studi\", but “better” → “better”).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NLTK Stemming (using two types of stemmers: Porter & Snowball) ===\n",
      "Token        Porter       Snowball    \n",
      "----------------------------------------\n",
      "The          the          the         \n",
      "striped      stripe       stripe      \n",
      "bats         bat          bat         \n",
      "were         were         were        \n",
      "hanging      hang         hang        \n",
      "on           on           on          \n",
      "their        their        their       \n",
      "feet         feet         feet        \n",
      "for          for          for         \n",
      "the          the          the         \n",
      "best         best         best        \n",
      "outcomes     outcom       outcom      \n",
      ".            .            .           \n",
      "They         they         they        \n",
      "had          had          had         \n",
      "been         been         been        \n",
      "running      run          run         \n",
      ",            ,            ,           \n",
      "thinking     think        think       \n",
      ",            ,            ,           \n",
      "and          and          and         \n",
      "eating       eat          eat         \n",
      "quickly      quickli      quick       \n",
      ".            .            .           \n"
     ]
    }
   ],
   "source": [
    "print(\"=== NLTK Stemming (using two types of stemmers: Porter & Snowball) ===\")\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "print(f\"{'Token':<12} {'Porter':<12} {'Snowball':<12}\")\n",
    "print(\"-\" * 40)\n",
    "for token in tokens:\n",
    "    print(f\"{token:<12} {porter.stem(token):<12} {snowball.stem(token):<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  NLTK Lemmatization using WordNet Lemmatizer\n",
    "\n",
    "- *What it does*: Uses the WordNet dictionary to convert words to their base (lemma) form.\n",
    "- *Pros*: More accurate than stemming; returns valid words.\n",
    "- *Cons*: Requires part-of-speech (POS) tagging for full accuracy, which must be added manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NLTK Lemmatization (WordNet) ===\n",
      "Token        Lemma       \n",
      "-------------------------\n",
      "The          The         \n",
      "striped      striped     \n",
      "bats         bat         \n",
      "were         were        \n",
      "hanging      hanging     \n",
      "on           on          \n",
      "their        their       \n",
      "feet         foot        \n",
      "for          for         \n",
      "the          the         \n",
      "best         best        \n",
      "outcomes     outcome     \n",
      ".            .           \n",
      "They         They        \n",
      "had          had         \n",
      "been         been        \n",
      "running      running     \n",
      ",            ,           \n",
      "thinking     thinking    \n",
      ",            ,           \n",
      "and          and         \n",
      "eating       eating      \n",
      "quickly      quickly     \n",
      ".            .           \n"
     ]
    }
   ],
   "source": [
    "print(\"=== NLTK Lemmatization (WordNet) ===\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(f\"{'Token':<12} {'Lemma':<12}\")\n",
    "print(\"-\" * 25)\n",
    "for token in tokens:\n",
    "    print(f\"{token:<12} {lemmatizer.lemmatize(token):<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy lemmatization\n",
    "\n",
    "-   *What it does*: Uses a context-aware NLP model with built-in POS tagging to return accurate lemmas.\n",
    "-   *Pros*: Very accurate, handles irregular forms and context well.\n",
    "-   *Cons*: Slightly slower than NLTK due to additional processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== spaCy Lemmatization ===\n",
      "Token        Lemma        \n",
      "-----------------------------------\n",
      "The          the          \n",
      "striped      striped      \n",
      "bats         bat          \n",
      "were         be           \n",
      "hanging      hang         \n",
      "on           on           \n",
      "their        their        \n",
      "feet         foot         \n",
      "for          for          \n",
      "the          the          \n",
      "best         good         \n",
      "outcomes     outcome      \n",
      ".            .            \n",
      "They         they         \n",
      "had          have         \n",
      "been         be           \n",
      "running      run          \n",
      ",            ,            \n",
      "thinking     think        \n",
      ",            ,            \n",
      "and          and          \n",
      "eating       eat          \n",
      "quickly      quickly      \n",
      ".            .            \n"
     ]
    }
   ],
   "source": [
    "print(\"=== spaCy Lemmatization ===\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "print(f\"{'Token':<12} {'Lemma':<12} \")\n",
    "print(\"-\" * 35)\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<12} {token.lemma_:<12} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary table\n",
    "\n",
    "| Technique        | tool       | what it does                                                                 |\n",
    "|------------------|------------|--------------------------------------------------------------------------------------------------|\n",
    "| **Stemming**     | **NLTK**       | Chops off word endings using simple rules. It’s fast, but the results can be rough or weird.     |\n",
    "| **Lemmatization**| **NLTK**       | Looks up words in a dictionary (WordNet) to get their base form, but doesn’t understand context. |\n",
    "| **Lemmatization**| **spaCy**  | Smart and accurate. Understands the meaning and grammar of the sentence to find the right base word. |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
