{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f03a124",
   "metadata": {},
   "source": [
    "# Week 2 Exercise: Stemming, lemmatization, and word cloud -- Answers\n",
    "\n",
    "This notebook contains questions and code templates to guide you through preprocessing text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d36436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e7f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load spaCy model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a7e6e7",
   "metadata": {},
   "source": [
    "### **1. Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8a486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset path and the source you want to read from\n",
    "dataset_path = 'articles/'\n",
    "source_name = 'Vox'  # Change to desired source if needed, like BBC or The Guardian\n",
    "\n",
    "# Correct the glob pattern to find files in the specified source folder across all dates\n",
    "newspaperfiles = glob(os.path.join(dataset_path, f'*/{source_name}/*'))\n",
    "\n",
    "# Initialize a list to hold documents\n",
    "documents = []\n",
    "\n",
    "# Read files and handle encoding errors if necessary\n",
    "for filename in newspaperfiles:\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            documents.append(f.read())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filename}: {e}\")\n",
    "\n",
    "print(f\"Loaded {len(documents)} articles from {source_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1634b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = documents[:100] ### Note that things slow down if you take a larger sample, so for practicing try it out on a small sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a01e68",
   "metadata": {},
   "source": [
    "before we can continue to stemming and lemmaitizaton, we should apply tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b10ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Tokenization\n",
    "print(\"\\n### Tokenization ###\")\n",
    "tokens = [word_tokenize(doc.lower()) for doc in documents]  # Convert to lowercase and tokenize\n",
    "print(\"First 20 tokens from the first document:\", tokens[0][:20])\n",
    "\n",
    "\n",
    "### Step 3: Stopword Removal ###\n",
    "print(\"\\n### Stopword Removal ###\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [[word for word in doc if word.isalnum() and word not in stop_words] for doc in tokens]\n",
    "print(\"Tokens after removing stopwords (first document):\", filtered_tokens[0][:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2104c793",
   "metadata": {},
   "source": [
    "### **2. Apply stemming and lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa31a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 4: Stemming and Lemmatization ###\n",
    "print(\"\\n### Stemming and Lemmatization ###\")\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stemmed_words = [[stemmer.stem(word) for word in doc] for doc in filtered_tokens]\n",
    "lemmatized_words_nltk = [[lemmatizer.lemmatize(word, pos='v') for word in doc] for doc in filtered_tokens]\n",
    "\n",
    "# Using spaCy for lemmatization\n",
    "lemmatized_words_spacy = []\n",
    "for doc in filtered_tokens:\n",
    "    spacy_doc = nlp(\" \".join(doc))\n",
    "    lemmatized_words_spacy.append([token.lemma_ for token in spacy_doc])\n",
    "    \n",
    "print(\"First 20 stemmed words (PorterStemmer, first document):\", stemmed_words[0][:20])\n",
    "print(\"First 20 lemmatized words (NLTK, first document):\", lemmatized_words_nltk[0][:20])\n",
    "print(\"First 20 lemmatized words (spaCy, first document):\", lemmatized_words_spacy[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef78c1b8",
   "metadata": {},
   "source": [
    "### 3. Generate word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1766f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### Generating Word Cloud ###\")\n",
    "all_lemmatized_text = \" \".join([\" \".join(doc) for doc in lemmatized_words_spacy])\n",
    "\n",
    "if all_lemmatized_text:\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_lemmatized_text)\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No words available to generate a word cloud.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
