{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5796410f",
   "metadata": {},
   "source": [
    "## Exercises Week 1: Working with textual data -- ANSWERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfa280d",
   "metadata": {},
   "source": [
    "In this assignment, you will work with textual data, focusing on dataset structure, processing, and basic analysis techniques. You will inspect the dataset, discuss research questions, and implement fundamental preprocessing steps such as tokenization, stopword removal, and stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cec0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10260074",
   "metadata": {},
   "source": [
    "### 1. Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f08b77",
   "metadata": {},
   "source": [
    "Download `articles.tar.gz` or `articles.zip` from Canvas (under Week 1). Unpack the dataset and inspect the contents.\n",
    "\n",
    "Hint: On Windows, you can use built-in extraction tools or `tar -xvzf articles.tar.gz `on macOS/Linux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9cad72",
   "metadata": {},
   "source": [
    "### 2. Inspect the structure of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ef6e65",
   "metadata": {},
   "source": [
    "What information do the following elements provide about the dataset?\n",
    "\n",
    "- Folder (directory) names\n",
    "- Folder structure/hierarchy\n",
    "- File names\n",
    "- File contents\n",
    "\n",
    "How can you programmatically inspect these aspects of the dataset?\n",
    "\n",
    "*Hint*: Consider using `os.listdir()` to check the folder contents and `glob` for pattern-based file selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f95a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'articles'\n",
    "\n",
    "folders = os.listdir(dataset_path)\n",
    "print(\"Folders:\", folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b823de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the contents of each folder to get an overview.\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\"Folder: {folder}\")\n",
    "        print(\"Contents:\", os.listdir(folder_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b3c7be",
   "metadata": {},
   "source": [
    "### 3. Discuss strategies for working with this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da81741b",
   "metadata": {},
   "source": [
    "Considering the dataset's size and structure:\n",
    "\n",
    " - Research questions: \n",
    "      * How do different news outlets cover the same event? Are there notable differences in tone or word choice?\n",
    "     *  What are the most frequently discussed topics across different dates and sources?\n",
    "      * Can we detect sentiment trends over time or between news outlets?\n",
    "      *  Is it possible to identify bias or framing through word frequency and topic modeling?\n",
    " - Strategies: Process files in batches to avoid memory overload, use generators to handle large datasets efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982bad13",
   "metadata": {},
   "source": [
    "### 4. Read some (or all) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceef2c4",
   "metadata": {},
   "source": [
    "Load a sample of the dataset and display the first few lines of text.\n",
    "How would you handle reading a large number of files efficiently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# Define dataset path and the source you want to read from\n",
    "dataset_path = 'articles/'\n",
    "source_name = 'Vox'  # Change to desired source if needed, like BBC or The Guardian\n",
    "\n",
    "# Correct the glob pattern to find files in the specified source folder across all dates\n",
    "newspaperfiles = glob(os.path.join(dataset_path, f'*/{source_name}/*'))\n",
    "\n",
    "# Initialize a list to hold documents\n",
    "documents = []\n",
    "\n",
    "# Read files and handle encoding errors if necessary\n",
    "for filename in newspaperfiles:\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            documents.append(f.read())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filename}: {e}\")\n",
    "\n",
    "print(f\"Loaded {len(documents)} articles from {source_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f59d7a",
   "metadata": {},
   "source": [
    "<div class=\"alert-block alert-warning\">\n",
    "  <p><strong>Tip:</strong> If you're looking to test or practice your code, it's a great idea to start by working with a random sample of the articles. This allows you to quickly check whether your logic works without having to process the entire dataset. Once you're confident that your code functions correctly on the smaller sample, you can easily scale up and apply it to the full set of documents.</p>\n",
    "  \n",
    "  <p>Here's a simple Python code snippet to help you randomly select a subset of articles for practice purposes:</p>\n",
    "  \n",
    "  <pre><code>import random\n",
    "articles = random.sample(documents, 10)  # Randomly select 10 articles</code></pre>\n",
    "  \n",
    "  <p>This will select 10 random articles from the 'documents' list, which you can then use for testing your code. Remember, as long as your code works on this smaller sample, you can confidently scale up and run it on the entire collection of documents when you're ready!</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b28277",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = random.sample(documents, 10) \n",
    "print(articles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3c06ee",
   "metadata": {},
   "source": [
    "## 5. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f44d88e",
   "metadata": {},
   "source": [
    "What is tokenization, and why is it useful in text processing?\n",
    "\n",
    "Implement a basic tokenization process using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0561a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First: try it out on a sample sentence\n",
    "\n",
    "text = 'This is a sample sentence for tokenization.'\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f4bcf2",
   "metadata": {},
   "source": [
    "You can experiment with different texts to observe how the word_tokenize function handles various punctuation, contractions, and other linguistic features. This will give you deeper insight into the way tokenization works in NLP tasks. For example, apply to the  `articles` you have just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06459d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Second: scale up to articles\n",
    "\n",
    "tokenized_articles = []\n",
    "\n",
    "for article in articles:\n",
    "    tokens = word_tokenize(article)\n",
    "    tokenized_articles.append(tokens)\n",
    "\n",
    "# Display the tokenized result for the first article as an example\n",
    "print(tokenized_articles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb4548",
   "metadata": {},
   "source": [
    "### 5. Stopword Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da093284",
   "metadata": {},
   "source": [
    "- Stopwords are common words that usually carry little meaning in text analysis, such as \"is\", \"and\", \"the\".\n",
    "- Removing them helps focus on more meaningful content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8631a5",
   "metadata": {},
   "source": [
    "We will demonstrate how to filter out stopwords from a sample sentence using the stopwords list from the NLTK library. The goal is to remove common words (like \"is\", \"a\", \"for\") that don't contribute much to the meaning of the sentence, leaving behind the more significant words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b51f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentence for tokenization and stopword removal\n",
    "text = 'This is a sample sentence for tokenization and stopword removal.'\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Get the set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Filter out stopwords from the tokens\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Display the filtered tokens\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679f1898",
   "metadata": {},
   "source": [
    "Now scale up to include more articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# List to store the filtered tokens for each article\n",
    "filtered_articles = []\n",
    "\n",
    "# Apply tokenization and stopword removal to each article\n",
    "for article in articles:\n",
    "    # Tokenize the article into words\n",
    "    tokens = word_tokenize(article)\n",
    "    \n",
    "    # Filter out stopwords from the tokens\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Append the filtered tokens to the result list\n",
    "    filtered_articles.append(filtered_tokens)\n",
    "\n",
    "# Display the filtered tokens for the first article as an example\n",
    "print(filtered_articles[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
